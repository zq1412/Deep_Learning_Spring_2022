{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q27g0pk2gbgl"
      },
      "source": [
        "**LeNet-5 Modified Without Batch Normalization or Dropout Layer**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZptxUkQHTSGD",
        "outputId": "19338c25-d569-4cfd-b567-1fbfa65af502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated.\u001b[0m\n",
            "==> Preparing data..\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.263818\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.392254\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.270179\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.305942\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.352823\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.367050\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.342188\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.267949\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.510047\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.636711\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.326707\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.478300\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.527300\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.247479\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.337989\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.472579\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.196864\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.391839\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.523150\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.320198\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.343980\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.522742\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.252509\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.235094\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.456867\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.554963\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.421588\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.716352\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.200354\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.200699\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.358107\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.304074\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.588426\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.262461\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.108196\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.228305\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.170485\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.340653\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.333137\n",
            "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.351795\n",
            "\n",
            "Test set: Average loss: 1.3546, Accuracy: 5164/10000 (52%)\n",
            "\n",
            "Traning and Testing total excution time is: 171.11405277252197 seconds \n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride = 2),\n",
        "            nn.Conv2d(16,120,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10),\n",
        "            nn.LogSoftmax(dim=-1))\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        y = self.convnet(x)\n",
        "        out = self.fc(y)\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "    \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 5\n",
        "    lr = 0.05\n",
        "    no_cuda = True\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wthBH1G9g10d"
      },
      "source": [
        "**LeNet-5 Modified With Dropout of 0.25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xSnnnsJWu1z",
        "outputId": "cbbddbd6-16e7-4bc5-d245-9b46b7eff3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated.\u001b[0m\n",
            "==> Preparing data..\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.482280\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.606092\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.472915\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.484362\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.385534\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.269458\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.592531\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.429612\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.339258\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.433643\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.528907\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.287607\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.329433\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.242911\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.509386\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.455746\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.464274\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.345242\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.379432\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.545887\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.638395\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.528017\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.520306\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.555760\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.344525\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.293481\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.450259\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.410293\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.532765\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.387138\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.430206\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.550483\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.311178\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.452378\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.597022\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.541989\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.395297\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.425033\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.395423\n",
            "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.380180\n",
            "\n",
            "Test set: Average loss: 1.3366, Accuracy: 5360/10000 (54%)\n",
            "\n",
            "Traning and Testing total excution time is: 180.29024577140808 seconds \n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25) #Dropout Layer\n",
        "            nn.MaxPool2d(2, stride = 2),\n",
        "            nn.Conv2d(16,120,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10),\n",
        "            nn.LogSoftmax(dim=-1))\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        y = self.convnet(x)\n",
        "        out = self.fc(y)\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "    \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 5\n",
        "    lr = 0.05\n",
        "    no_cuda = True\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3GJzj52mg-nT"
      },
      "source": [
        "**LeNet-5 Modified with Batch Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOu2qm5rbX5e",
        "outputId": "9bc9f934-e2ef-4b53-eba7-68fc025274e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated.\u001b[0m\n",
            "==> Preparing data..\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.239151\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.316419\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.323324\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.203551\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.323869\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.249375\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.268979\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.284709\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.404243\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.381832\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.309171\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.185022\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.356520\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.344690\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.016199\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.366272\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.044989\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.211726\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.362775\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.200444\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.330957\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.302507\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.162808\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.229841\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.451375\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.370911\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.200535\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.331817\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.172724\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.223089\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.299269\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.204544\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.345490\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.172475\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.260045\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.217262\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.278877\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.265257\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.382376\n",
            "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.478799\n",
            "\n",
            "Test set: Average loss: 1.2320, Accuracy: 5643/10000 (56%)\n",
            "\n",
            "Traning and Testing total excution time is: 185.22702407836914 seconds \n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.BatchNorm2d(6), #Batch Normalization Layer\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride = 2),\n",
        "            nn.Conv2d(16,120,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10),\n",
        "            nn.LogSoftmax(dim=-1))\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        y = self.convnet(x)\n",
        "        \n",
        "        out = self.fc(y)\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "    \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 5\n",
        "    lr = 0.05\n",
        "    no_cuda = True\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The network without a drop out layer or batch normalization performed the worst with a test accuracy of 52% over 5 epochs. The network with a dropout layer of 0.25 performed the second best with a test accuracy of 54%. The network with batch normalization performed the best with an accuracy of 56% over 5 epochs. As far as the training time is concerned, the plain network was the fastest with a training time of 171 seconds. The network with the dropout layer was the second slowest with a training time of 180 seconds. The network with the batch normalization was the slowest with a training time of 185 seconds. Screenshots of the testing loss are provided. If you wish to see training loss, expand the raw output data in this file."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Homework_4_Deep_Learning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
