{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q27g0pk2gbgl"
      },
      "source": [
        "**LeNet-5 Modified Without Batch Normalization or Dropout Layer**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZptxUkQHTSGD",
        "outputId": "19338c25-d569-4cfd-b567-1fbfa65af502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.306809\n",
            "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.291311\n",
            "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.227425\n",
            "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.136307\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.015715\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.018222\n",
            "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.008607\n",
            "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 1.954003\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.912950\n",
            "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 2.082973\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.819596\n",
            "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.907707\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.746715\n",
            "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.790530\n",
            "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.882849\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.858795\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.740097\n",
            "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.893781\n",
            "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.861095\n",
            "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.919297\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.765083\n",
            "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.653740\n",
            "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.841604\n",
            "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.722860\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.794723\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.669613\n",
            "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.655451\n",
            "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.653257\n",
            "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.792260\n",
            "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.727394\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.735734\n",
            "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.833299\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.879379\n",
            "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.595366\n",
            "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.683248\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.715070\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.659002\n",
            "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.663903\n",
            "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.673884\n",
            "Train Epoch: 1 [31200/50000 (100%)]\tLoss: 1.490973\n",
            "\n",
            "Test set: Average loss: 1.5304, Accuracy: 4453/10000 (45%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.513805\n",
            "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.547149\n",
            "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.561978\n",
            "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.651863\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.561598\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.685422\n",
            "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.863986\n",
            "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.694121\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.726895\n",
            "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.605370\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.546949\n",
            "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.517146\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.570383\n",
            "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.761167\n",
            "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.598013\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.610928\n",
            "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.476441\n",
            "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.598876\n",
            "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.594449\n",
            "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.529667\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.549475\n",
            "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.569613\n",
            "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.555768\n",
            "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.507685\n",
            "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.529511\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.625704\n",
            "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.514882\n",
            "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.594217\n",
            "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.694562\n",
            "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.609277\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.506495\n",
            "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.479337\n",
            "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.533466\n",
            "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.522600\n",
            "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.658483\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.564604\n",
            "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.441774\n",
            "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.431171\n",
            "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.545862\n",
            "Train Epoch: 2 [31200/50000 (100%)]\tLoss: 1.621776\n",
            "\n",
            "Test set: Average loss: 1.4700, Accuracy: 4814/10000 (48%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.400589\n",
            "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.526697\n",
            "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.541745\n",
            "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 1.506853\n",
            "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.561937\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.607744\n",
            "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.379387\n",
            "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 1.544027\n",
            "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.609527\n",
            "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 1.452166\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.622010\n",
            "Train Epoch: 3 [14080/50000 (28%)]\tLoss: 1.601860\n",
            "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.499030\n",
            "Train Epoch: 3 [16640/50000 (33%)]\tLoss: 1.619068\n",
            "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.524017\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.463460\n",
            "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.426983\n",
            "Train Epoch: 3 [21760/50000 (43%)]\tLoss: 1.466639\n",
            "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.428475\n",
            "Train Epoch: 3 [24320/50000 (49%)]\tLoss: 1.439673\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.575634\n",
            "Train Epoch: 3 [26880/50000 (54%)]\tLoss: 1.287040\n",
            "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 1.450153\n",
            "Train Epoch: 3 [29440/50000 (59%)]\tLoss: 1.565473\n",
            "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.626145\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.302202\n",
            "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.385809\n",
            "Train Epoch: 3 [34560/50000 (69%)]\tLoss: 1.485435\n",
            "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 1.362635\n",
            "Train Epoch: 3 [37120/50000 (74%)]\tLoss: 1.667857\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.247142\n",
            "Train Epoch: 3 [39680/50000 (79%)]\tLoss: 1.651715\n",
            "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.362002\n",
            "Train Epoch: 3 [42240/50000 (84%)]\tLoss: 1.515767\n",
            "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.430804\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.440150\n",
            "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.312876\n",
            "Train Epoch: 3 [47360/50000 (95%)]\tLoss: 1.410356\n",
            "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 1.492707\n",
            "Train Epoch: 3 [31200/50000 (100%)]\tLoss: 1.420948\n",
            "\n",
            "Test set: Average loss: 1.3370, Accuracy: 5316/10000 (53%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.309403\n",
            "Train Epoch: 4 [1280/50000 (3%)]\tLoss: 1.399650\n",
            "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.420010\n",
            "Train Epoch: 4 [3840/50000 (8%)]\tLoss: 1.499991\n",
            "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.324336\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.298384\n",
            "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 1.240304\n",
            "Train Epoch: 4 [8960/50000 (18%)]\tLoss: 1.498164\n",
            "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.481754\n",
            "Train Epoch: 4 [11520/50000 (23%)]\tLoss: 1.259282\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.689401\n",
            "Train Epoch: 4 [14080/50000 (28%)]\tLoss: 1.524513\n",
            "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.589813\n",
            "Train Epoch: 4 [16640/50000 (33%)]\tLoss: 1.613810\n",
            "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.312370\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.368647\n",
            "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.546353\n",
            "Train Epoch: 4 [21760/50000 (43%)]\tLoss: 1.437249\n",
            "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.668829\n",
            "Train Epoch: 4 [24320/50000 (49%)]\tLoss: 1.408668\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.495809\n",
            "Train Epoch: 4 [26880/50000 (54%)]\tLoss: 1.447275\n",
            "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.463167\n",
            "Train Epoch: 4 [29440/50000 (59%)]\tLoss: 1.337771\n",
            "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.558496\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.510890\n",
            "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 1.315096\n",
            "Train Epoch: 4 [34560/50000 (69%)]\tLoss: 1.333522\n",
            "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 1.328959\n",
            "Train Epoch: 4 [37120/50000 (74%)]\tLoss: 1.553424\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.474967\n",
            "Train Epoch: 4 [39680/50000 (79%)]\tLoss: 1.627225\n",
            "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.384387\n",
            "Train Epoch: 4 [42240/50000 (84%)]\tLoss: 1.213204\n",
            "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 1.335904\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.614261\n",
            "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.342474\n",
            "Train Epoch: 4 [47360/50000 (95%)]\tLoss: 1.392730\n",
            "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 1.415872\n",
            "Train Epoch: 4 [31200/50000 (100%)]\tLoss: 1.522389\n",
            "\n",
            "Test set: Average loss: 1.3370, Accuracy: 5307/10000 (53%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.263818\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.392254\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.270179\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.305942\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.352823\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.367050\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.342188\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.267949\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.510047\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.636711\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.326707\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.478300\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.527300\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.247479\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.337989\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.472579\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.196864\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.391839\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.523150\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.320198\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.343980\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.522742\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.252509\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.235094\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.456867\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.554963\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.421588\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.716352\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.200354\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.200699\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.358107\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.304074\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.588426\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.262461\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.108196\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.228305\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.170485\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.340653\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.333137\n",
            "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.351795\n",
            "\n",
            "Test set: Average loss: 1.3546, Accuracy: 5164/10000 (52%)\n",
            "\n",
            "Traning and Testing total excution time is: 171.11405277252197 seconds \n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride = 2),\n",
        "            nn.Conv2d(16,120,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10),\n",
        "            nn.LogSoftmax(dim=-1))\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        y = self.convnet(x)\n",
        "        out = self.fc(y)\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "    \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 5\n",
        "    lr = 0.05\n",
        "    no_cuda = True\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wthBH1G9g10d"
      },
      "source": [
        "**LeNet-5 Modified With Dropout of 0.25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xSnnnsJWu1z",
        "outputId": "cbbddbd6-16e7-4bc5-d245-9b46b7eff3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.305384\n",
            "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.287058\n",
            "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.192961\n",
            "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.114276\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.090188\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.928884\n",
            "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.969677\n",
            "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.046214\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.888032\n",
            "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.840638\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.882881\n",
            "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.991352\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.758903\n",
            "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.884164\n",
            "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.857030\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.818795\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.702539\n",
            "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.940417\n",
            "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.814002\n",
            "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.775100\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.827857\n",
            "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.699517\n",
            "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.743014\n",
            "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.852040\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.844499\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.776858\n",
            "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.677835\n",
            "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.666071\n",
            "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.718714\n",
            "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.762814\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.583405\n",
            "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.714166\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.745107\n",
            "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.610079\n",
            "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.705964\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.672031\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.725326\n",
            "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.719524\n",
            "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.573494\n",
            "Train Epoch: 1 [31200/50000 (100%)]\tLoss: 1.646807\n",
            "\n",
            "Test set: Average loss: 1.5414, Accuracy: 4350/10000 (44%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.767309\n",
            "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.586506\n",
            "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.673856\n",
            "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.515685\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.927455\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.624276\n",
            "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.728734\n",
            "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.526377\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.704588\n",
            "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.822994\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.572979\n",
            "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.644779\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.506712\n",
            "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.543919\n",
            "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.659828\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.601165\n",
            "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.637158\n",
            "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.569324\n",
            "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.913828\n",
            "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.744684\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.949949\n",
            "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.570156\n",
            "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.826662\n",
            "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.640472\n",
            "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.583298\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.684646\n",
            "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.525463\n",
            "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.683233\n",
            "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.716936\n",
            "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.646630\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.418597\n",
            "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.668644\n",
            "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.467727\n",
            "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.538340\n",
            "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.498428\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.678998\n",
            "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.631163\n",
            "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.418739\n",
            "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.593833\n",
            "Train Epoch: 2 [31200/50000 (100%)]\tLoss: 1.636345\n",
            "\n",
            "Test set: Average loss: 1.5048, Accuracy: 4573/10000 (46%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.598256\n",
            "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.664106\n",
            "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.707700\n",
            "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 1.703843\n",
            "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.750635\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.622498\n",
            "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.588822\n",
            "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 1.551833\n",
            "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.412658\n",
            "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 1.622003\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.470490\n",
            "Train Epoch: 3 [14080/50000 (28%)]\tLoss: 1.583318\n",
            "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.614258\n",
            "Train Epoch: 3 [16640/50000 (33%)]\tLoss: 1.573289\n",
            "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.427879\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.646989\n",
            "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.493684\n",
            "Train Epoch: 3 [21760/50000 (43%)]\tLoss: 1.367004\n",
            "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.468294\n",
            "Train Epoch: 3 [24320/50000 (49%)]\tLoss: 1.516962\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.509314\n",
            "Train Epoch: 3 [26880/50000 (54%)]\tLoss: 1.571742\n",
            "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 1.529149\n",
            "Train Epoch: 3 [29440/50000 (59%)]\tLoss: 1.619030\n",
            "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.765289\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.384833\n",
            "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.344156\n",
            "Train Epoch: 3 [34560/50000 (69%)]\tLoss: 1.585105\n",
            "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 1.731880\n",
            "Train Epoch: 3 [37120/50000 (74%)]\tLoss: 1.493473\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.418106\n",
            "Train Epoch: 3 [39680/50000 (79%)]\tLoss: 1.392336\n",
            "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.523880\n",
            "Train Epoch: 3 [42240/50000 (84%)]\tLoss: 1.429208\n",
            "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.656395\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.507688\n",
            "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.456251\n",
            "Train Epoch: 3 [47360/50000 (95%)]\tLoss: 1.376616\n",
            "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 1.436558\n",
            "Train Epoch: 3 [31200/50000 (100%)]\tLoss: 1.520317\n",
            "\n",
            "Test set: Average loss: 1.4549, Accuracy: 4869/10000 (49%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.392815\n",
            "Train Epoch: 4 [1280/50000 (3%)]\tLoss: 1.522639\n",
            "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.654581\n",
            "Train Epoch: 4 [3840/50000 (8%)]\tLoss: 1.657654\n",
            "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.406022\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.441074\n",
            "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 1.628526\n",
            "Train Epoch: 4 [8960/50000 (18%)]\tLoss: 1.739832\n",
            "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.615368\n",
            "Train Epoch: 4 [11520/50000 (23%)]\tLoss: 1.551462\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.422302\n",
            "Train Epoch: 4 [14080/50000 (28%)]\tLoss: 1.556121\n",
            "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.440692\n",
            "Train Epoch: 4 [16640/50000 (33%)]\tLoss: 1.531766\n",
            "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.362885\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.447071\n",
            "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.580042\n",
            "Train Epoch: 4 [21760/50000 (43%)]\tLoss: 1.699040\n",
            "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.683364\n",
            "Train Epoch: 4 [24320/50000 (49%)]\tLoss: 1.538232\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.712202\n",
            "Train Epoch: 4 [26880/50000 (54%)]\tLoss: 1.436506\n",
            "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.248281\n",
            "Train Epoch: 4 [29440/50000 (59%)]\tLoss: 1.442958\n",
            "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.510195\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.565678\n",
            "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 1.492207\n",
            "Train Epoch: 4 [34560/50000 (69%)]\tLoss: 1.393127\n",
            "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 1.366571\n",
            "Train Epoch: 4 [37120/50000 (74%)]\tLoss: 1.535263\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.432457\n",
            "Train Epoch: 4 [39680/50000 (79%)]\tLoss: 1.594353\n",
            "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.477476\n",
            "Train Epoch: 4 [42240/50000 (84%)]\tLoss: 1.406265\n",
            "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 1.349888\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.322163\n",
            "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.430899\n",
            "Train Epoch: 4 [47360/50000 (95%)]\tLoss: 1.451428\n",
            "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 1.380453\n",
            "Train Epoch: 4 [31200/50000 (100%)]\tLoss: 1.378664\n",
            "\n",
            "Test set: Average loss: 1.3637, Accuracy: 5132/10000 (51%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.482280\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.606092\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.472915\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.484362\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.385534\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.269458\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.592531\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.429612\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.339258\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.433643\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.528907\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.287607\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.329433\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.242911\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.509386\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.455746\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.464274\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.345242\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.379432\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.545887\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.638395\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.528017\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.520306\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.555760\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.344525\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.293481\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.450259\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.410293\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.532765\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.387138\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.430206\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.550483\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.311178\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.452378\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.597022\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.541989\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.395297\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.425033\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.395423\n",
            "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.380180\n",
            "\n",
            "Test set: Average loss: 1.3366, Accuracy: 5360/10000 (54%)\n",
            "\n",
            "Traning and Testing total excution time is: 180.29024577140808 seconds \n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25) #Dropout Layer\n",
        "            nn.MaxPool2d(2, stride = 2),\n",
        "            nn.Conv2d(16,120,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10),\n",
        "            nn.LogSoftmax(dim=-1))\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        y = self.convnet(x)\n",
        "        out = self.fc(y)\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "    \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 5\n",
        "    lr = 0.05\n",
        "    no_cuda = True\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3GJzj52mg-nT"
      },
      "source": [
        "**LeNet-5 Modified with Batch Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOu2qm5rbX5e",
        "outputId": "9bc9f934-e2ef-4b53-eba7-68fc025274e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.308962\n",
            "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.292907\n",
            "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.227754\n",
            "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.298429\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.106616\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.082714\n",
            "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.969209\n",
            "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.111538\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.895650\n",
            "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.896650\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.831419\n",
            "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.820726\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.737247\n",
            "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.861948\n",
            "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.874434\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.837155\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.783866\n",
            "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 2.032804\n",
            "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.883677\n",
            "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.850715\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.731177\n",
            "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.635033\n",
            "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.820302\n",
            "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.825780\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.805423\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.672807\n",
            "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.646159\n",
            "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.682420\n",
            "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.918242\n",
            "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.711938\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.737404\n",
            "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.678810\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.868997\n",
            "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.688792\n",
            "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.697991\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.713282\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.512949\n",
            "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.554307\n",
            "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.623284\n",
            "Train Epoch: 1 [31200/50000 (100%)]\tLoss: 1.560083\n",
            "\n",
            "Test set: Average loss: 1.4810, Accuracy: 4616/10000 (46%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.319474\n",
            "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.590696\n",
            "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.508831\n",
            "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.514170\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.546506\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.606923\n",
            "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.740849\n",
            "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.524959\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.603218\n",
            "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.629901\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.612192\n",
            "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.438924\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.469701\n",
            "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.689051\n",
            "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.438553\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.443408\n",
            "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.383918\n",
            "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.655055\n",
            "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.421586\n",
            "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.448504\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.515590\n",
            "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.490111\n",
            "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.484643\n",
            "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.432733\n",
            "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.477565\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.546908\n",
            "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.434193\n",
            "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.467090\n",
            "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.603994\n",
            "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.554685\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.422071\n",
            "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.340794\n",
            "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.374748\n",
            "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.407583\n",
            "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.533482\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.552853\n",
            "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.432932\n",
            "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.365121\n",
            "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.481332\n",
            "Train Epoch: 2 [31200/50000 (100%)]\tLoss: 1.604375\n",
            "\n",
            "Test set: Average loss: 1.3616, Accuracy: 5071/10000 (51%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.392279\n",
            "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.530106\n",
            "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.491543\n",
            "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 1.506498\n",
            "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.418194\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.512456\n",
            "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.435326\n",
            "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 1.466332\n",
            "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.391283\n",
            "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 1.365587\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.514908\n",
            "Train Epoch: 3 [14080/50000 (28%)]\tLoss: 1.527569\n",
            "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.495685\n",
            "Train Epoch: 3 [16640/50000 (33%)]\tLoss: 1.389198\n",
            "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.431456\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.377917\n",
            "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.399183\n",
            "Train Epoch: 3 [21760/50000 (43%)]\tLoss: 1.400179\n",
            "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.435307\n",
            "Train Epoch: 3 [24320/50000 (49%)]\tLoss: 1.300802\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.467291\n",
            "Train Epoch: 3 [26880/50000 (54%)]\tLoss: 1.241139\n",
            "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 1.300551\n",
            "Train Epoch: 3 [29440/50000 (59%)]\tLoss: 1.560305\n",
            "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.384373\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.520208\n",
            "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.340269\n",
            "Train Epoch: 3 [34560/50000 (69%)]\tLoss: 1.566401\n",
            "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 1.389986\n",
            "Train Epoch: 3 [37120/50000 (74%)]\tLoss: 1.616684\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.305949\n",
            "Train Epoch: 3 [39680/50000 (79%)]\tLoss: 1.506307\n",
            "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.287255\n",
            "Train Epoch: 3 [42240/50000 (84%)]\tLoss: 1.462201\n",
            "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.360207\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.456360\n",
            "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.373216\n",
            "Train Epoch: 3 [47360/50000 (95%)]\tLoss: 1.314221\n",
            "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 1.320985\n",
            "Train Epoch: 3 [31200/50000 (100%)]\tLoss: 1.299770\n",
            "\n",
            "Test set: Average loss: 1.3136, Accuracy: 5311/10000 (53%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.139406\n",
            "Train Epoch: 4 [1280/50000 (3%)]\tLoss: 1.399280\n",
            "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.315288\n",
            "Train Epoch: 4 [3840/50000 (8%)]\tLoss: 1.372167\n",
            "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.242113\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.306151\n",
            "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 1.264941\n",
            "Train Epoch: 4 [8960/50000 (18%)]\tLoss: 1.439797\n",
            "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.566741\n",
            "Train Epoch: 4 [11520/50000 (23%)]\tLoss: 1.260813\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.483652\n",
            "Train Epoch: 4 [14080/50000 (28%)]\tLoss: 1.372175\n",
            "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.446682\n",
            "Train Epoch: 4 [16640/50000 (33%)]\tLoss: 1.341089\n",
            "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.183801\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.395398\n",
            "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.306003\n",
            "Train Epoch: 4 [21760/50000 (43%)]\tLoss: 1.259070\n",
            "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.412583\n",
            "Train Epoch: 4 [24320/50000 (49%)]\tLoss: 1.369055\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.351956\n",
            "Train Epoch: 4 [26880/50000 (54%)]\tLoss: 1.368358\n",
            "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.287252\n",
            "Train Epoch: 4 [29440/50000 (59%)]\tLoss: 1.353214\n",
            "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.416607\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.359871\n",
            "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 1.347879\n",
            "Train Epoch: 4 [34560/50000 (69%)]\tLoss: 1.227867\n",
            "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 1.180778\n",
            "Train Epoch: 4 [37120/50000 (74%)]\tLoss: 1.419079\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.463858\n",
            "Train Epoch: 4 [39680/50000 (79%)]\tLoss: 1.300766\n",
            "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.284792\n",
            "Train Epoch: 4 [42240/50000 (84%)]\tLoss: 1.234408\n",
            "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 1.266505\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.398336\n",
            "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.280270\n",
            "Train Epoch: 4 [47360/50000 (95%)]\tLoss: 1.152112\n",
            "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 1.246479\n",
            "Train Epoch: 4 [31200/50000 (100%)]\tLoss: 1.316049\n",
            "\n",
            "Test set: Average loss: 1.2939, Accuracy: 5392/10000 (54%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.239151\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.316419\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.323324\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.203551\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.323869\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.249375\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.268979\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.284709\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.404243\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.381832\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.309171\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.185022\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.356520\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.344690\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.016199\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.366272\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.044989\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.211726\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.362775\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.200444\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.330957\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.302507\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.162808\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.229841\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.451375\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.370911\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.200535\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.331817\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 1.172724\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.223089\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.299269\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 1.204544\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 1.345490\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.172475\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 1.260045\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.217262\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.278877\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.265257\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.382376\n",
            "Train Epoch: 5 [31200/50000 (100%)]\tLoss: 1.478799\n",
            "\n",
            "Test set: Average loss: 1.2320, Accuracy: 5643/10000 (56%)\n",
            "\n",
            "Traning and Testing total excution time is: 185.22702407836914 seconds \n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            nn.BatchNorm2d(6), #Batch Normalization Layer\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(6,16,5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride = 2),\n",
        "            nn.Conv2d(16,120,5),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten())\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(120,84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,10),\n",
        "            nn.LogSoftmax(dim=-1))\n",
        "\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        y = self.convnet(x)\n",
        "        \n",
        "        out = self.fc(y)\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "    \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        ############################\n",
        "        #### Put your code here ####\n",
        "        ############################\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        ###########################\n",
        "        #### End of your codes ####\n",
        "        ###########################\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 5\n",
        "    lr = 0.05\n",
        "    no_cuda = True\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The network without a drop out layer or batch normalization performed the worst with a test accuracy of 52% over 5 epochs. The network with a dropout layer of 0.25 performed the second best with a test accuracy of 54%. The network with batch normalization performed the best with an accuracy of 56% over 5 epochs. As far as the training time is concerned, the plain network was the fastest with a training time of 171 seconds. The network with the dropout layer was the second slowest with a training time of 180 seconds. The network with the batch normalization was the slowest with a training time of 185 seconds. Screenshots of the testing loss are provided. If you wish to see training loss, expand the raw output data in this file."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Homework_4_Deep_Learning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
